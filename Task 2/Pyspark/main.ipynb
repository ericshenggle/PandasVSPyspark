{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "The dataset is comprised of tab-separated files with phrases from the IMDB Movie Ratings. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n",
    "\n",
    "test.tsv contains just phrases. You must assign a sentiment label to each phrase. The sentiment labels are:\n",
    "\n",
    "- 0 : negative \n",
    "- 1 : positive\n",
    "\n",
    "## Objective:\n",
    "- cleanup (if required).\n",
    "- Build classification models to predict the ratings of the movie implemented using the following algorithms:\n",
    "    - Logistic Regression\n",
    "    - Naive Bayes\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "    \n",
    "    ...\n",
    "- Implement the above algorithms with Pyspark.\n",
    "- Compare the evaluation metrics of vaious classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "memory = '20g'\n",
    "pyspark_submit_args = '--executor-memory 4g' + ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, NaiveBayes, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd\n",
    "\n",
    "# Initial SparkSession\n",
    "conf = SparkConf() \\\n",
    "    .set('spark.driver.host', '127.0.0.1') \\\n",
    "    .set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\") \\\n",
    "    .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .setMaster(\"local\").setAppName('MovieReview Sentiment Analysis')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "task_names = ['Load', 'Preprocess', 'LR Train', 'LR Evaluate', 'NB Train', 'NB Evaluate', 'DT Train', 'DT Evaluate', 'RF Train', 'RF Evaluate']\n",
    "task_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_csv('../Datasets/movie.csv', header=0)\n",
    "\n",
    "# Converting the dataset to spark dataframe\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "# Showing the first 5 rows of the dataframe\n",
    "# df.show(5)\n",
    "# print(f'Inference: The Dataset consists of {len(df.columns)} features & {df.count()} samples.')\n",
    "\n",
    "# Printing the schema of the dataframe\n",
    "# df.printSchema()\n",
    "\n",
    "# Checking the stats of the dataframe\n",
    "# df.describe().show()\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Checking for null values\n",
    "null_check = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# null_check.show()\n",
    "\n",
    "# print('Inference: The dataset doesn\\'t have any null elements' if not null_check.drop('label').toPandas().sum().sum() else '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "original_count = df.count()\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# check if duplicates are removed\n",
    "# if df.count() == original_count:\n",
    "#     print('Inference: The dataset doesn\\'t have any duplicates')\n",
    "# else:\n",
    "#     print(f'Inference: Number of duplicates dropped/fixed ---> {original_count - df.count()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP pipeline\n",
    "\n",
    "Spark NLP is a Natural Language Processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment. We can use Spark NLP to preprocess text data, like removing stop words, normalizing text, tokenizing. In this way, we can get a clean text data for further processing.\n",
    "\n",
    "We will be using Spark NLP pipeline to preprocess the data. The pipeline consists of the following stages:\n",
    "- DocumentAssembler: Converts the input string to Spark NLP internal structure (Annotation)\n",
    "- Tokenizer: Split the document into tokens (words)\n",
    "- Normalizer: Transforms tokens to Normalized form (lower case, accent removal, etc.)\n",
    "- Lemmatizer: Break tokens down to their root meaning to identify similar words (car, cars, car's => car)\n",
    "- Stemmer: Reduce words to their root form (write, writes, writing => writ)\n",
    "- Finisher: Clean up internal structures and convert to human readable output (Reverts back to text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator.token.tokenizer import Tokenizer\n",
    "from sparknlp.annotator.normalizer import Normalizer\n",
    "from sparknlp.annotator.lemmatizer import LemmatizerModel\n",
    "from sparknlp.annotator.stemmer import Stemmer\n",
    "from sparknlp.annotator.stop_words_cleaner import StopWordsCleaner\n",
    "from sparknlp.annotator.n_gram_generator import NGramGenerator\n",
    "\n",
    "# Create all the required annotators\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "        .setInputCol('text') \\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "# Tokenize the document \n",
    "tokenizer = Tokenizer() \\\n",
    "        .setInputCols(['document']) \\\n",
    "        .setOutputCol('tokenized')\n",
    "\n",
    "# Normalize the tokens\n",
    "normalizer = Normalizer() \\\n",
    "        .setInputCols(['tokenized']) \\\n",
    "        .setOutputCol('normalized') \\\n",
    "        .setLowercase(True)\n",
    "\n",
    "# Stem the normalized tokens\n",
    "stemmer = Stemmer() \\\n",
    "        .setInputCols(['normalized']) \\\n",
    "        .setOutputCol('stemmed')\n",
    "\n",
    "# Remove stopwords from the lemmatized tokens\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "        .setInputCols(['stemmed']) \\\n",
    "        .setOutputCol('unigrams') \\\n",
    "        .setStopWords(eng_stopwords)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams'])\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,                  \n",
    "                 tokenizer,\n",
    "                 normalizer,        \n",
    "                 stemmer,                  \n",
    "                 stopwords_cleaner, \n",
    "                 finisher])\n",
    "\n",
    "# Transform DataFrame\n",
    "df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show the first 5 rows of the dataframe after filtering\n",
    "# df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the text data\n",
    "\n",
    "Before we start building the models, we need to convert the text data into numerical data. We will be using TF-IDF vectorizer to convert the text data into numerical data. \n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# create a TF vectorizer\n",
    "hashing_TF = HashingTF(inputCol='finished_unigrams', outputCol='tf_features')\n",
    "tf_df = hashing_TF.transform(df)\n",
    "\n",
    "# create an IDF estimator\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = tfidf_df.select(['label', 'tf_idf_features'])\n",
    "# Ensure that the labels are of float type\n",
    "feature_df = feature_df.withColumn(\"label\", feature_df[\"label\"].cast(\"float\"))\n",
    "# feature_df.show(5)\n",
    "\n",
    "# Split the data\n",
    "(train_data, test_data) = feature_df.randomSplit([0.8, 0.2], seed=0)\n",
    "\n",
    "# Show the number of samples in each set\n",
    "# print('Original set  ---> ', feature_df.count(), len(feature_df.columns))\n",
    "# print('Training set  ---> ', train_data.count(), len(train_data.columns))\n",
    "# print('Testing set   ---> ', test_data.count(), len(test_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the metrics for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the evaluation results of each model\n",
    "Evaluation_Results = {\n",
    "    'Logistic Regression (LR)': [0] * 5,\n",
    "    'Decision Tree Classifier (DT)': [0] * 5,\n",
    "    'Random Forest Classifier (RF)': [0] * 5,\n",
    "    'Naïve Bayes Classifier (NB)': [0] * 5\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a pandas DataFrame\n",
    "Evaluation_Results_df = pd.DataFrame(Evaluation_Results, index=['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC score']).T\n",
    "Evaluation_Results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function for model evaluation\n",
    "\n",
    "Classification evaluation metrics are used to access the performance of a classifier. The metrics are used to access the performance of the model on the test set. The metrics that we will be using are:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "- ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Classification Summary Function\n",
    "def classification_summary(predictions, model_name):\n",
    "    print('\\nConfusion Matrix:')\n",
    "    # Compute the confusion matrix\n",
    "    rdd = predictions.select(['prediction', 'label']).rdd.map(tuple)    \n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    # Compute various metrics\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[0][1])\n",
    "    recall = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[1][0])\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # Compute AUC-ROC\n",
    "    # Convert predictions to Pandas DataFrame\n",
    "    predictions_pd = predictions.select(\"probability\", \"label\").toPandas()\n",
    "\n",
    "    # Extract the probability of the positive class\n",
    "    prob_positive_class = predictions_pd.apply(lambda row: row['probability'][1], axis=1)\n",
    "\n",
    "    # Compute FPR, TPR, and AUC-ROC\n",
    "    fpr, tpr, _ = roc_curve(predictions_pd['label'], prob_positive_class)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "\n",
    "    # Update the results dataframe\n",
    "    Evaluation_Results_df.loc[model_name] = [accuracy*100, precision*100, recall*100, f1*100, auc_roc*100]\n",
    "    \n",
    "    # Print the results\n",
    "    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*35, model_name, '-'*35,'>'*3))\n",
    "    print('Accuracy = {:.2f}%'.format(accuracy * 100))\n",
    "    print('Precision = {:.2f}%'.format(precision * 100))\n",
    "    print('Recall = {:.2f}%'.format(recall * 100))\n",
    "    print('F1 Score = {:.2f}%'.format(f1 * 100))\n",
    "    print('AUC-ROC = {:.2f}%'.format(auc_roc * 100))\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the applications of Logistic Regression are:\n",
    "- Image Segmentation and Categorization\n",
    "- Geographic Image Processing\n",
    "- Handwriting recognition\n",
    "- Healthcare: Analyzing a group of over million people for myocardial infarction within a period of 10 years is an application area of logistic regression.\n",
    "\n",
    "Logistic Regression is a special case of Linear Regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. It basically predicts the probability of occurrence of an event by fitting data to a logit function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "lr = LogisticRegression(featuresCol=\"tf_idf_features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Show the results\n",
    "classification_summary(lr_predictions, 'Logistic Regression (LR)')\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is a classification algorithm based on Bayes’ Theorem. It is termed as ‘Naive’ because it assumes independence between every pair of feature in the data. Let’s understand the working of Naive Bayes through an example. Suppose we have a dataset of weather conditions and corresponding target variable ‘Play’ indicating whether or not the match will be played. Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "nb = NaiveBayes(featuresCol=\"tf_idf_features\", labelCol=\"label\")\n",
    "nb_model = nb.fit(train_data)\n",
    "nb_predictions = nb_model.transform(test_data)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Show the results\n",
    "classification_summary(nb_predictions, 'Naïve Bayes Classifier (NB)')\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Decision Tree is a basic classification and regression algorithm. It is a supervised learning algorithm that can be used for both classification and regression. And it is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this algorithm, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(featuresCol=\"tf_idf_features\", labelCol=\"label\")\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Show the results\n",
    "classification_summary(dt_predictions, 'Decision Tree Classifier (DT)')\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random Forest is a classic machine learning ensemble method that is a popular choice in data science. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging. Bagging is a technique where a subset of the dataset is selected randomly, and a model is built on top of this subset. This model is then used to make predictions on the entire dataset. The predictions from each model are then combined using a simple majority vote (for classification) or average (for regression) to get the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"tf_idf_features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# # Show the results\n",
    "classification_summary(rf_predictions, 'Random Forest Classifier (RF)')\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the time taken for each task\n",
    "\n",
    "with open('time.txt', 'w') as f:\n",
    "    for i in range(len(task_names)):\n",
    "        f.write(f'{task_names[i]}: {task_times[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5344",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
