{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "The dataset is comprised of tab-separated files with phrases from the IMDB Movie Ratings. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n",
    "test.tsv contains just phrases. You must assign a sentiment label to each phrase.\n",
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "1 - positive\n",
    "\n",
    "### Objective:\n",
    "- Understand the Dataset & cleanup (if required).\n",
    "- Build classification models to predict the ratings of the movie.\n",
    "- Compare the evaluation metrics of vaious classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:11:05.215676975Z",
     "start_time": "2023-11-16T18:11:05.157324512Z"
    },
    "execution": {
     "iopub.execute_input": "2022-04-19T19:38:15.780569Z",
     "iopub.status.busy": "2022-04-19T19:38:15.780193Z",
     "iopub.status.idle": "2022-04-19T19:38:17.736209Z",
     "shell.execute_reply": "2022-04-19T19:38:17.735429Z",
     "shell.execute_reply.started": "2022-04-19T19:38:15.780479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing the necessary librarires\n",
    "\n",
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import scipy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, \\\n",
    "roc_auc_score, roc_curve, precision_score, recall_score\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "task_names = ['Load', 'Preprocess', 'LR Train', 'LR Evaluate', 'NB Train', 'NB Evaluate', 'DT Train', 'DT Evaluate', 'RF Train', 'RF Evaluate']\n",
    "task_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:11:05.543738553Z",
     "start_time": "2023-11-16T18:11:05.160953495Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T11:46:27.80843Z",
     "iopub.status.busy": "2022-01-19T11:46:27.808126Z",
     "iopub.status.idle": "2022-01-19T11:46:28.401063Z",
     "shell.execute_reply": "2022-01-19T11:46:28.398188Z",
     "shell.execute_reply.started": "2022-01-19T11:46:27.808387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mInference:\u001b[0m The Datset consists of 2 features & 40000 samples.\n"
     ]
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "\n",
    "df = pd.read_csv('../Datasets/movie.csv', header=0)\n",
    "target = 'label'\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "original_df = df.copy(deep=True)\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** The stats seem to be fine, let us gain more undestanding by visualising the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:11:05.544253776Z",
     "start_time": "2023-11-16T18:11:05.539455221Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T11:46:28.454014Z",
     "iopub.status.busy": "2022-01-19T11:46:28.453655Z",
     "iopub.status.idle": "2022-01-19T11:46:28.469885Z",
     "shell.execute_reply": "2022-01-19T11:46:28.468853Z",
     "shell.execute_reply.started": "2022-01-19T11:46:28.453966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "\u001b[1mInference:\u001b[0m The dataset doesn't have any null elements\n"
     ]
    }
   ],
   "source": [
    "#Check for empty elements\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any null elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:11:05.619619369Z",
     "start_time": "2023-11-16T18:11:05.539703439Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T11:46:28.472212Z",
     "iopub.status.busy": "2022-01-19T11:46:28.471343Z",
     "iopub.status.idle": "2022-01-19T11:46:28.636744Z",
     "shell.execute_reply": "2022-01-19T11:46:28.636108Z",
     "shell.execute_reply.started": "2022-01-19T11:46:28.47216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Removal of any Duplicate rows (if any)\n",
    "\n",
    "counter = 0\n",
    "r,c = original_df.shape\n",
    "\n",
    "df1 = df.drop_duplicates()\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# if df1.shape==(r,c):\n",
    "#     print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\n",
    "# else:\n",
    "#     print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped/fixed ---> {r-df1.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:20:23.507731469Z",
     "start_time": "2023-11-16T18:11:05.623880142Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T11:46:28.638463Z",
     "iopub.status.busy": "2022-01-19T11:46:28.63767Z",
     "iopub.status.idle": "2022-01-19T12:13:20.524205Z",
     "shell.execute_reply": "2022-01-19T12:13:20.523369Z",
     "shell.execute_reply.started": "2022-01-19T11:46:28.638423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39723/39723 [09:17<00:00, 71.21it/s] \n"
     ]
    }
   ],
   "source": [
    "#Filtering the text\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "df = df1.copy()\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([i for i in text if i in string.ascii_lowercase+' '])\n",
    "    text = ' '.join([PorterStemmer().stem(word) for word in text.split()])\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "#with Pool(4) as p:\n",
    "#    df['text'] = list(tqdm(p.imap(preprocessor, range(df.shape[0]))))\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    df.loc[i,'text'] = preprocessor(df['text'][i])\n",
    "\n",
    "#from tqdm.contrib.concurrent import process_map \n",
    "\n",
    "#df['text'] = process_map(tqdm(preprocessor, df['text'], max_workers=8))\n",
    "\n",
    "#for i in tqdm()\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** The text is now clean up with the removal of all punctuations, stopwords & stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:10.491790303Z",
     "start_time": "2023-11-16T18:20:23.507538707Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:13:20.526492Z",
     "iopub.status.busy": "2022-01-19T12:13:20.525622Z",
     "iopub.status.idle": "2022-01-19T12:15:45.098215Z",
     "shell.execute_reply": "2022-01-19T12:15:45.097614Z",
     "shell.execute_reply.started": "2022-01-19T12:13:20.526445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "def tokenizer(text):\n",
    "        return text.split()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)\n",
    "y=df.label.values\n",
    "x=tfidf.fit_transform(df.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 3. Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:10.492206556Z",
     "start_time": "2023-11-16T18:21:10.491536920Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:17:10.663543Z",
     "iopub.status.busy": "2022-01-19T12:17:10.663124Z",
     "iopub.status.idle": "2022-01-19T12:17:10.693184Z",
     "shell.execute_reply": "2022-01-19T12:17:10.692358Z",
     "shell.execute_reply.started": "2022-01-19T12:17:10.663503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original set  --->  (39723, 1) (39723,) \n",
      "Training set  --->  (31778, 119535) (31778,) \n",
      "Testing set   --->  (7945, 119535)  (7945,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data intro training & testing sets\n",
    "\n",
    "X = df.drop([target],axis=1)\n",
    "Y = df[target]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:10.494643869Z",
     "start_time": "2023-11-16T18:21:10.491910849Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:17:10.694901Z",
     "iopub.status.busy": "2022-01-19T12:17:10.694487Z",
     "iopub.status.idle": "2022-01-19T12:17:10.710781Z",
     "shell.execute_reply": "2022-01-19T12:17:10.709887Z",
     "shell.execute_reply.started": "2022-01-19T12:17:10.694869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC-ROC score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (LR)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (DT)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (RF)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naïve Bayes Classifier (NB)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Precision  Recall  F1-score  \\\n",
       "Logistic Regression (LR)            0.0        0.0     0.0       0.0   \n",
       "Decision Tree Classifier (DT)       0.0        0.0     0.0       0.0   \n",
       "Random Forest Classifier (RF)       0.0        0.0     0.0       0.0   \n",
       "Naïve Bayes Classifier (NB)         0.0        0.0     0.0       0.0   \n",
       "\n",
       "                               AUC-ROC score  \n",
       "Logistic Regression (LR)                 0.0  \n",
       "Decision Tree Classifier (DT)            0.0  \n",
       "Random Forest Classifier (RF)            0.0  \n",
       "Naïve Bayes Classifier (NB)              0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us create first create a table to store the results of various models \n",
    "\n",
    "Evaluation_Results = pd.DataFrame(np.zeros((4,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\n",
    "Evaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Naïve Bayes Classifier (NB)']\n",
    "Evaluation_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:10.495568565Z",
     "start_time": "2023-11-16T18:21:10.492070575Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:17:10.713434Z",
     "iopub.status.busy": "2022-01-19T12:17:10.712671Z",
     "iopub.status.idle": "2022-01-19T12:17:10.728331Z",
     "shell.execute_reply": "2022-01-19T12:17:10.727484Z",
     "shell.execute_reply.started": "2022-01-19T12:17:10.713378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Let us define functions to summarise the Prediction's scores .\n",
    "\n",
    "#Classification Summary Function\n",
    "def Classification_Summary(pred,pred_prob,i):\n",
    "    Evaluation_Results.iloc[i]['Accuracy']=round(accuracy_score(Test_Y, pred),3)*100   \n",
    "    Evaluation_Results.iloc[i]['Precision']=round(precision_score(Test_Y, pred),3)*100 #, average='weighted'\n",
    "    Evaluation_Results.iloc[i]['Recall']=round(recall_score(Test_Y, pred),3)*100 #, average='weighted'\n",
    "    Evaluation_Results.iloc[i]['F1-score']=round(f1_score(Test_Y, pred),3)*100 #, average='weighted'\n",
    "    Evaluation_Results.iloc[i]['AUC-ROC score']=round(roc_auc_score(Test_Y, pred),3)*100 #, multi_class='ovr'\n",
    "    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*35,Evaluation_Results.index[i], '-'*35,'>'*3))\n",
    "    print('Accuracy = {}%'.format(round(accuracy_score(Test_Y, pred),3)*100))\n",
    "    print('F1 Score = {}%'.format(round(f1_score(Test_Y, pred),3)*100)) #, average='weighted'\n",
    "    print('\\n \\033[1mConfusiton Matrix:\\033[0m\\n',confusion_matrix(Test_Y, pred))\n",
    "    print('\\n\\033[1mClassification Report:\\033[0m\\n',classification_report(Test_Y, pred))\n",
    "    \n",
    "\n",
    "#Visualising Function\n",
    "def AUC_ROC_plot(Test_Y, pred):    \n",
    "    ref = [0 for _ in range(len(Test_Y))]\n",
    "    ref_auc = roc_auc_score(Test_Y, ref)\n",
    "    lr_auc = roc_auc_score(Test_Y, pred)\n",
    "\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(Test_Y, ref)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(Test_Y, pred)\n",
    "\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:12.425205326Z",
     "start_time": "2023-11-16T18:21:10.492297140Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:19:48.095826Z",
     "iopub.status.busy": "2022-01-19T12:19:48.095541Z",
     "iopub.status.idle": "2022-01-19T12:19:53.736902Z",
     "shell.execute_reply": "2022-01-19T12:19:53.73602Z",
     "shell.execute_reply.started": "2022-01-19T12:19:48.095793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<-----------------------------------\u001b[1m Evaluating Logistic Regression (LR) \u001b[0m----------------------------------->>>\n",
      "\n",
      "Accuracy = 89.1%\n",
      "F1 Score = 89.0%\n",
      "\n",
      " \u001b[1mConfusiton Matrix:\u001b[0m\n",
      " [[3584  500]\n",
      " [ 367 3494]]\n",
      "\n",
      "\u001b[1mClassification Report:\u001b[0m\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      4084\n",
      "           1       0.87      0.90      0.89      3861\n",
      "\n",
      "    accuracy                           0.89      7945\n",
      "   macro avg       0.89      0.89      0.89      7945\n",
      "weighted avg       0.89      0.89      0.89      7945\n"
     ]
    }
   ],
   "source": [
    "# Building Logistic Regression Classifier\n",
    "\n",
    "LR_model = LogisticRegression()\n",
    "LR = LR_model.fit(Train_X, Train_Y)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "pred = LR.predict(Test_X)\n",
    "pred_prob = LR.predict_proba(Test_X)\n",
    "Classification_Summary(pred,pred_prob,0)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Naive Bayes Classfier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building Naive Bayes Classifier\n",
    "\n",
    "NB_model = BernoulliNB()\n",
    "NB = NB_model.fit(Train_X, Train_Y)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "pred = NB.predict(Test_X)\n",
    "pred_prob = NB.predict_proba(Test_X)\n",
    "Classification_Summary(pred,pred_prob,3)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decisoin Tree Classfier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:21:54.123702904Z",
     "start_time": "2023-11-16T18:21:12.427082584Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:19:53.739441Z",
     "iopub.status.busy": "2022-01-19T12:19:53.738953Z",
     "iopub.status.idle": "2022-01-19T12:20:59.971543Z",
     "shell.execute_reply": "2022-01-19T12:20:59.970672Z",
     "shell.execute_reply.started": "2022-01-19T12:19:53.739389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<-----------------------------------\u001b[1m Evaluating Decision Tree Classifier (DT) \u001b[0m----------------------------------->>>\n",
      "\n",
      "Accuracy = 70.8%\n",
      "F1 Score = 69.89999999999999%\n",
      "\n",
      " \u001b[1mConfusiton Matrix:\u001b[0m\n",
      " [[2925 1159]\n",
      " [1161 2700]]\n",
      "\n",
      "\u001b[1mClassification Report:\u001b[0m\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      4084\n",
      "           1       0.70      0.70      0.70      3861\n",
      "\n",
      "    accuracy                           0.71      7945\n",
      "   macro avg       0.71      0.71      0.71      7945\n",
      "weighted avg       0.71      0.71      0.71      7945\n"
     ]
    }
   ],
   "source": [
    "# Building Decision Tree Classifier\n",
    "\n",
    "DT_model = DecisionTreeClassifier()\n",
    "DT = DT_model.fit(Train_X, Train_Y)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "pred = DT.predict(Test_X)\n",
    "pred_prob = DT.predict_proba(Test_X)\n",
    "Classification_Summary(pred,pred_prob,1)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Classfier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T18:23:19.357207128Z",
     "start_time": "2023-11-16T18:21:54.123503787Z"
    },
    "execution": {
     "iopub.execute_input": "2022-01-19T12:20:59.973004Z",
     "iopub.status.busy": "2022-01-19T12:20:59.972771Z",
     "iopub.status.idle": "2022-01-19T12:23:30.279675Z",
     "shell.execute_reply": "2022-01-19T12:23:30.278763Z",
     "shell.execute_reply.started": "2022-01-19T12:20:59.972976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<-----------------------------------\u001b[1m Evaluating Random Forest Classifier (RF) \u001b[0m----------------------------------->>>\n",
      "\n",
      "Accuracy = 84.5%\n",
      "F1 Score = 84.2%\n",
      "\n",
      " \u001b[1mConfusiton Matrix:\u001b[0m\n",
      " [[3438  646]\n",
      " [ 582 3279]]\n",
      "\n",
      "\u001b[1mClassification Report:\u001b[0m\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      4084\n",
      "           1       0.84      0.85      0.84      3861\n",
      "\n",
      "    accuracy                           0.85      7945\n",
      "   macro avg       0.85      0.85      0.85      7945\n",
      "weighted avg       0.85      0.85      0.85      7945\n"
     ]
    }
   ],
   "source": [
    "# Building Random Forest Classifier\n",
    "\n",
    "RF_model = RandomForestClassifier()\n",
    "RF = RF_model.fit(Train_X, Train_Y)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "pred = RF.predict(Test_X)\n",
    "pred_prob = RF.predict_proba(Test_X)\n",
    "Classification_Summary(pred,pred_prob,2)\n",
    "\n",
    "task_times.append(datetime.datetime.now() - start_time)\n",
    "start_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the time taken for each task\n",
    "\n",
    "with open('time.txt', 'w') as f:\n",
    "    for i in range(len(task_names)):\n",
    "        f.write(f'{task_names[i]}: {task_times[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
